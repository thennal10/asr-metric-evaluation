{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the language and whether to normalize the transcripts/ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM = False\n",
    "LANG = \"en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if LANG == \"ml\":\n",
    "    file_path = \"survey/survey_malayalam.csv\"\n",
    "elif LANG == \"en\":\n",
    "    file_path = \"survey/survey_english.csv\"\n",
    "elif LANG == \"ar\":\n",
    "    file_path = \"survey/survey_arabic.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_path = f\"transcriptions/{LANG}/ground.txt\"\n",
    "ground = pd.read_csv(ground_path, sep=\"|\", header=None)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate and Modify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below loads the phonemizer, for calculating Phoneme Error Rate (PER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonemizer import phonemize\n",
    "from phonemizer.separator import Separator\n",
    "from phonemizer.backend import EspeakBackend\n",
    "from difflib import ndiff\n",
    "import re\n",
    "\n",
    "\n",
    "separator = Separator(phone='-', word=';')\n",
    "backend = EspeakBackend(LANG if LANG != \"en\" else \"en-us\")\n",
    "\n",
    "def phonemizer(text):\n",
    "    phonemes = backend.phonemize([text], separator=separator)\n",
    "    phonemes = [e for e in re.split(r'[-;]', phonemes[0]) if e]\n",
    "    return phonemes\n",
    "\n",
    "def per(str1, str2):\n",
    "    phonemes1 = phonemizer(str1)\n",
    "    phonemes2 = phonemizer(str2)\n",
    "\n",
    "    counter = {\"+\": 0, \"-\": 0}\n",
    "    distance = 0\n",
    "    for edit_code, *_ in ndiff(phonemes1, phonemes2):\n",
    "        if edit_code == \" \":\n",
    "            distance += max(counter.values())\n",
    "            counter = {\"+\": 0, \"-\": 0}\n",
    "        else: \n",
    "            if edit_code != '?':\n",
    "                counter[edit_code] += 1\n",
    "    \n",
    "    distance += max(counter.values())\n",
    "    return distance/len(phonemes1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates all metrics and collates them into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer, cer\n",
    "\n",
    "if NORM:\n",
    "    from whisper.normalizers import EnglishTextNormalizer\n",
    "    norm = EnglishTextNormalizer()\n",
    "\n",
    "data = []\n",
    "for i in range(50):\n",
    "    question = []\n",
    "    for j in range(4):\n",
    "        q = df[f\"Q{i+1}_{j+1}\"]\n",
    "        trans, scores = q[0], q[1:]\n",
    "        if NORM:\n",
    "            trans = norm(trans)\n",
    "        subdata = {\n",
    "            'transcript': trans,\n",
    "            'scores': [float(s) for s in scores],\n",
    "            'wer': wer(ground[i], trans),\n",
    "            'cer': cer(ground[i], trans),\n",
    "            'per': per(ground[i], trans)\n",
    "        }\n",
    "        question.append(subdata)        \n",
    "    data.append(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between the raw evaluator scores and the metrics is calculated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Correlation\n",
      "wer: 52.99\n",
      "cer: 54.69\n",
      "per: 52.63\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "scores = np.array([[list(x['scores']) for x in q] for q in data])\n",
    "scores_rank = rankdata(scores, method='average', axis=1)\n",
    "\n",
    "metrics = {}\n",
    "metrics_rank = {}\n",
    "metrics_flat = {}\n",
    "for met in ['wer', 'cer', 'per']:\n",
    "    metrics[met] = np.array([[x[met] for x in q] for q in data])\n",
    "    # make metric into 50 x 4 x num_evaluators by copying\n",
    "    metrics[met] = np.tile(metrics[met].reshape(50, 4, 1), (1, 1, scores.shape[-1]))\n",
    "    metrics_rank[met] = rankdata(metrics[met], method='average', axis=1)\n",
    "    metrics_flat[met] = metrics[met].flatten()\n",
    "\n",
    "# flatten scores\n",
    "scores_flat = scores.flatten()\n",
    "\n",
    "print(\"Score Correlation\")\n",
    "for met in ['wer', 'cer', 'per']:\n",
    "    print(f\"{met}: {round(np.corrcoef(metrics_flat[met], scores_flat)[0, 1]*-100, 2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank correlation between the rankings of the metrics and the evaluator scores is calculated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spearman's Rank Correlation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wer: 68.51\n",
      "cer: 73.47\n",
      "per: 43.12\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr, ttest_rel\n",
    "from itertools import combinations\n",
    "\n",
    "print(\"\\nSpearman's Rank Correlation\")\n",
    "spearmanr_metric = {}\n",
    "for met in ['wer', 'cer', 'per']:\n",
    "    spearmanr_metric[met] = []\n",
    "    for (s_arr, m_arr) in zip(scores_rank, metrics_rank[met]):\n",
    "        for s, m in zip(s_arr.T, m_arr.T):\n",
    "            spearmanr_metric[met].append(spearmanr(s, m)[0])\n",
    "    \n",
    "    # replace nan with 0\n",
    "    spearmanr_metric[met] = [0 if np.isnan(x) else x for x in spearmanr_metric[met]]\n",
    "\n",
    "    print(f\"{met}: {round(np.mean(spearmanr_metric[met]) * -100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the T-test p-value to determine if the correlation difference between the two metrics is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "T-test using Spearman's Rank Correlation\n",
      "wer vs cer: 1.1073872458614983e-12\n",
      "wer vs per: 1.0\n",
      "cer vs per: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nT-test using Spearman's Rank Correlation\")\n",
    "for met_comb in combinations(['wer', 'cer', 'per'], 2):\n",
    "    rv1 = spearmanr_metric[met_comb[0]]\n",
    "    rv2 = spearmanr_metric[met_comb[1]]\n",
    "    ttest_res = ttest_rel(rv1, rv2, alternative='greater')\n",
    "    print(f\"{met_comb[0]} vs {met_comb[1]}: {ttest_res.pvalue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kendalll's W is an interrater reliability metric calculated between the 20 evaluators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendall's W\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6211"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kendall_w(expt_ratings):\n",
    "    # calculate correction for ties\n",
    "    T = []\n",
    "    for q in expt_ratings:\n",
    "        Tq = 0\n",
    "        for sub in q.T:\n",
    "            _,  counts = np.unique(sub, axis=0, return_counts=True)\n",
    "            Tq += (counts**3 - counts).sum()\n",
    "        T.append(Tq)\n",
    "    T = np.array(T)\n",
    "    \n",
    "    m = expt_ratings.shape[2] #raters\n",
    "    n = expt_ratings.shape[1] # items rated\n",
    "    denom = m**2*(n**3-n) - m*T\n",
    "    R = np.sum(expt_ratings.T, axis=0)\n",
    "    nom = 12 * np.sum(R**2, axis=0) - 3*m**2*n*(n+1)**2\n",
    "    W = nom/denom\n",
    "    return W.mean()\n",
    "\n",
    "print(\"Kendall's W\")\n",
    "round(kendall_w(scores_rank), 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "random",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
